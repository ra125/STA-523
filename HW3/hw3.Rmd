---
title: "hw3"
author: "Radhika Anand, Sophie Lee, Minjung Park, Kungang Zhang"
date: "10/19/2014"
output: html_document
---
## Task 1 : Geocoding

1) Load the biggish NYC parking violation datafile by using a (fread) function which is much faster and more convenient than a (read.table) function 

2) Extract the necessary variables

Using 'setnames' we changed some variable names. Then using the library 'dplyr', first we chose a part of [Violation.Precinct] data which is less than 34 by filtering. Second, by using a (mutate) function, we added two variables [House.number, Street.Name], erasing whitespace in the data of two variables. Then we took the non-empty data and chose [House.Number] data which has a "[0-9]+" pattern by using a (filter) function. Third, we combined the [Violation.Precinct] variable with the [House.Number,Street.Name] variable which is called "addr" by using transmute function and then lastly converted the case to lower.

3) Load 'pluto data' to geocode by using 'readShapeSpatial' which reads data from a spatial dataframe to a shapefile.

We made a dataset called 'tax' which combines coordinates of 'pluto' data with address of it. Then we called the address of 'pluto' data 'addr' which is the same variable as we used in the dataframe above so as to implement inner_join on them.

3) Clearing messy data

First, we made a combined vector called 'namemap' which includes the full name ("east","west","street","avenue","av","avee","road","drive","terrace","bway","terrace"," terr","bway","1st","2nd","3rd","th ") and the short name ("e","w","st","ave","ave","ave","rd","dr","ter","brdway","ter"," ter","brdway","1","2","3"," ") and then, in the for loop, we replaced the full name with the short name in both 'tax' data from pluto and 'addr' data from park. We used 'str_match_all' to do so. So after cleaning a few words and removing suffix from numbers, we could atch upto 1.5 mil rows. 

```{r}
#Load necessary packages
source("check_packages.R")
check_packages(c("data.table","rgeos","ggmap","rgdal","maptools", "dplyr","stringr","lubridate","e1071", "RColorBrewer"))

#Load the Biggish NYC parking violations datafile
options(warn=-1)
park = fread("/home/vis/cr173/Sta523/data/parking/NYParkingViolations.csv",sep=",", verbose=FALSE, showProgress=FALSE)

#Rename required columns
setnames(park, "Violation Precinct", "Violation.Precinct")
setnames(park, "House Number", "House.Number")
setnames(park, "Street Name", "Street.Name")

#Extract the necessary variables
addr = filter(park, Violation.Precinct <= 34) %>%
  mutate(House.Number = str_trim(House.Number), Street.Name = str_trim(Street.Name)) %>%
  filter(House.Number != "" & Street.Name != "") %>%
  filter(str_detect(House.Number,"[0-9]+")) %>%
  transmute(Violation.Precinct = Violation.Precinct, addr = paste(House.Number, Street.Name)) %>%
  mutate(addr = tolower(addr))

#Load 'pluto' data to geocode
basepath<-getwd()
setwd("/home/vis/cr173/Sta523/data/parking/pluto/Manhattan/")
pluto<-readShapeSpatial("MNMapPLUTO")
setwd(basepath)
rm(basepath)

tax = cbind(data.frame(coordinates(pluto)), tolower(as.character(pluto$Address)))

names(tax)[3] = "addr"

#from full name to simplified version, because this makes str_replace_all much easier
namemap<-rbind(c("east","west","street","avenue","av","avee","road","drive","terrace"," terr","bway","1st","2nd","3rd","th "), c("e","w","st","ave","ave","ave","rd","dr","ter"," ter","brdway","1","2","3"," "))
lnm<-ncol(namemap)

for (i in 1:lnm)
{
  tax$addr<-str_replace_all(tax$addr, namemap[1,i], namemap[2,i])
  addr$addr<-str_replace_all(addr$addr, namemap[1,i], namemap[2,i])
}

#Around 1.5 mil matches after this clean up
```

4) Next, we used a more comprehensive approach to clean the data where we saw, each time, the number of unmatched rows in pluto and then tried to clean the words in those rows. We did this but it was taking a lot of time and space and was increasing the matching by a very small and insignificant amount. Hence, we decided to not go ahead wth it and so we have not included its code.

5) Joining addr and tax using 'inner_join' and removing park and pluto

```{r}
# Join the dataframe
z = inner_join(tax,addr)

rm(park)
rm(pluto)
```

6) Plotting the precincts on map in different colors to get an idea of the boundaries. We see that the concentrated points are very good but we have a lot of outliers to get rid off.

```{r,fig.align='center'}
latlon=data.frame(cbind(z$X1, z$X2))
names(latlon$X1)="lon"
names(latlon$X2)="lat"
coord=SpatialPoints(latlon)
plot(coord, col=z$Violation.Precinct, pch=18, cex=0.5, axes=TRUE)
dim(latlon)
```

## Task 2 : Recreating NYCâ€™s Police Precincts

1) We use the actual precinct numbers from NYPD website to subset the actual Manhattan precincts between 0 to 34. We got the list as 1,5,6,7,9,10,13,14,17,18,19,20,22,23,24,25,26,28,30,32,33,34 where Midtown South is 14, Manhattan North is 18, Central park is 22.

2) We got rid of outliers by taking the 90% quantile of lat/long pairs. Since points are numeric we can use the quantile function which sorts and removes 10% extreme outliers for each precinct.

3) We then made separate convex hulls for each precinct (restoring the precinct number for later use) using the function 'gConvexHull' from 'rgeos' library. We then merged all the convex hulls and made a single geojson file called 'precinct.json'. We got good results in this draft and a score of 378281.

4) Then we tried other approaches to create the boundaries like Alpha Hull and SVM. For alpha hull we used the package 'alphahull' and the functions 'ahull' and 'delvor'. We saw that with the full data it was taking a very long time to run and hence we subsetted a random 10 percent sample of the data and set 'alpha' value=0.01. We created precinct boundaries and plotted them on the map and saw that they were not significantly better because we were not using the boundary of full Manhattan anywhere and hence a lot of points close to the boundary were not getting included in the hulls. We thus havent included the code in this file but have the code saved with us.

5) Then we moved on to the final approach of using Support Vector Machines. We first removed 1 percent extreme outliers from each precinct by using the quantile function. Next we took a 3 percent random sample of the data for implementing SVM in a timely fashion for all precincts instead of 22. We did not sample from precinct 22 as we anyway had very less points for it. Instead we joined the points for precinct 22, 5 times in the dataframe so as to increase the points for this precinct (Central Park). We used the nybb file here to subset the points that were outside the Manhattan boundary. We used the function 'svm' from package 'e1071' to create the support vectors and then dissolved all points of the same precinct in one polygon to get the final precincts. The code for SVM is as below. We used cross=5 for cross validation. We tried tweaking other parameters of function 'svm' but couldnt get any better. We also tried the 'tune.svm' function but it was taking a very long time to run and hence we did not use it in the final code. The final precinct boundaries are stored in the spatial file 'precinct.json' that is created as a result of this code. 

```{r}
nybb = readOGR(path.expand("/home/vis/cr173/Sta523/data/parking/nybb/"),"nybb",stringsAsFactors=FALSE)
manh = nybb[2,]

true_p=c(1,5,6,7,9,10,13,14,17,18,19,20,22,23,24,25,26,28,30,32,33,34)

names(z)[2]='x'
names(z)[3]='y'

z=subset(z,(z$Violation.Precinct %in% true_p))

z1<-as.data.frame(z[which(z$Violation.Precinct==1),])
q_lon<-matrix(quantile(z1$x, probs=c(0.005, 0.995), na.rm=TRUE))
q_lat<-matrix(quantile(z1$y, probs=c(0.005, 0.995), na.rm=TRUE))
pres<-which(q_lon[1,1]< z1$x & z1$x<q_lon[2,1] & q_lat[1,1]< z1$y & z1$y<q_lat[2,1])
z1<-z1[pres,]

true_pp=c(5,6,7,9,10,13,14,17,18,19,20,23,24,25,26,28,30,32,33,34)

for(precinct in true_pp)
{
z2<-as.data.frame(z[which(z$Violation.Precinct==precinct),])
q_lon<-matrix(quantile(z2$x, probs=c(0.005, 0.995), na.rm=TRUE))
q_lat<-matrix(quantile(z2$y, probs=c(0.005, 0.995), na.rm=TRUE))
pres<-which(q_lon[1,1]< z2$x & z2$x<q_lon[2,1] & q_lat[1,1]< z2$y & z2$y<q_lat[2,1])
z2<-z2[pres,]
z1=rbind(z1,z2)
}

z22=z[z$Violation.Precinct==22,]
q_lon<-matrix(quantile(z22$x, probs=c(0.005, 0.995), na.rm=TRUE))
q_lat<-matrix(quantile(z22$y, probs=c(0.005, 0.995), na.rm=TRUE))
pres<-which(q_lon[1,1]< z22$x & z22$x<q_lon[2,1] & q_lat[1,1]< z22$y & z22$y<q_lat[2,1])
z22<-z22[pres,]

z=z1

n=nrow(z)
set.seed(1000)
sampled<-sample(1:n,as.integer(n*0.03), replace=FALSE)
z_true=z[sampled,]
z_true<-rbind(z_true,z22,z22,z22,z22,z22)
z_true=z_true[,2:4]

k=svm(as.factor(Violation.Precinct)~.,data=z_true,cross=5)

library(raster, quietly=TRUE, warn.conflicts=FALSE)
r = rasterize(manh, raster(ncols=500,nrows=1000,ext=extent(bbox(manh))))

cells = which(!is.na(r[]))
crds = xyFromCell(r,cells)

w = predict(k,crds)

r[cells] = as.numeric(as.character(w))

preci = sort(unique(z_true$Violation.Precinct))

l=list()
for(i in seq_along(preci))
{
  l[[i]] = rasterToPolygons(r, function(x) x==preci[i], dissolve=TRUE)
  l[[i]]@polygons[[1]]@ID = as.character(preci[i])
  rownames(l[[i]]@data) = preci[i]
  colnames(l[[i]]@data) = "Precinct"
}

pd = do.call(rbind, l)
plot(pd, col = z_true$Violation.Precinct)

source("write_json.R")
writeGeoJSON(pd, "./precinct.json")

```

